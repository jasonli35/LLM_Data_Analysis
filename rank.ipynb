{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('data_4072.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eval_name', 'Precision', 'Type', 'T', 'Weight type', 'Architecture',\n",
       "       'Model', 'fullname', 'Model sha', 'Average ‚¨ÜÔ∏è', 'Hub License', 'Hub ‚ù§Ô∏è',\n",
       "       '#Params (B)', 'Available on the hub', 'MoE', 'Flagged',\n",
       "       'Chat Template', 'CO‚ÇÇ cost (kg)', 'IFEval Raw', 'IFEval', 'BBH Raw',\n",
       "       'BBH', 'MATH Lvl 5 Raw', 'MATH Lvl 5', 'GPQA Raw', 'GPQA', 'MUSR Raw',\n",
       "       'MUSR', 'MMLU-PRO Raw', 'MMLU-PRO', 'Merged', 'Official Providers',\n",
       "       'Upload To Hub Date', 'Submission Date', 'Generation', 'Base Model'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['üí¨', 'üü¢', 'üî∂', 'ü§ù', 'üü©', 'üå∏', '‚ùì'], dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"T\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['üí¨ chat models (RLHF, DPO, IFT, ...)', 'üü¢ pretrained',\n",
       "       'üî∂ fine-tuned on domain-specific datasets',\n",
       "       'ü§ù base merges and moerges', 'üü© continuously pretrained',\n",
       "       'üå∏ multimodal', '‚ùì other'], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in df[\"Hub License\"] if it is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_rank(num_criteria = [], confident=0.5, num_top_model=5, str_criteria = []):\n",
    "    \"\"\"\n",
    "    The function will rank the models based on selected criteria. If the criteria is empty, we will take the average of all the metrics.\n",
    "    @param criteria: The metrics that is most important for this specific tasks. If it is None, we will take the average of these matrics.\n",
    "    @param confident: how much confident user think num_criteria and str_criteria are correct in range of 0-1.\n",
    "    @param num_top_model: The number of top models we want to show.\n",
    "    @param str_criteria: The criteria \n",
    "\n",
    "    @return: The rank of the models.\n",
    "    \"\"\"\n",
    "    assert type(num_criteria) == list, \"criteria should be a list\"\n",
    "    # matrices = [\"MMLU-PRO\", \"MUSR\", \"GPQA\", \"MATH Lvl 5\", \"BBH\", \"IFEval\", \"CO‚ÇÇ cost (kg)\", \"Flagged\", \"MoE\", \"#Params (B)\", \"Hub ‚ù§Ô∏è\"]\n",
    "    matrices = [\"MMLU-PRO\", \"MUSR\", \"GPQA\", \"MATH Lvl 5\", \"BBH\", \"IFEval\", \"CO‚ÇÇ cost (kg)\", \"Flagged\", \"MoE\", \"#Params (B)\", \"Hub ‚ù§Ô∏è\"]\n",
    "    for c in num_criteria:\n",
    "        assert c in matrices, f\"criteria {c} is not allowed. Note that it can only take one of the following values: ['MMLU-PRO', 'MUSR', 'GPQA', 'MATH Lvl 5', 'BBH', 'IFEval', 'CO‚ÇÇ cost (kg)', 'Flagged', 'MoE', '#Params (B)', 'Hub ‚ù§Ô∏è']\" \n",
    "    assert type(confident) == float, \"weight should be a float\"\n",
    "    assert 0 <= confident <= 1, \"weight should be between 0 and 1\"\n",
    " \n",
    "    assert type(str_criteria) == list, \"str_criteria should be a list\"\n",
    "    for c in str_criteria:\n",
    "        assert type(c) == tuple, \"Entry of the str_criteria should be a tuple\"\n",
    "        assert c[0] in df.columns, f\"criteria {c[0]} is not allowed. Note that it can only take one of the following values: {df.columns}\"\n",
    "        assert c[1] in df[c[0]].unique(), f\"criteria {c[1]} is not allowed for {c[0]}. Note that it can only take one of the following values: {df[c[0]].unique()}\"\n",
    "\n",
    "    MAX_SCORE = 100\n",
    "    \n",
    "\n",
    "    this_df = df.copy()\n",
    "    this_df[\"new_score\"] = 0    \n",
    "\n",
    "    #add more condition\n",
    "    if len(num_criteria) == 0: \n",
    "        this_df[\"new_score\"] = df[\"Average ‚¨ÜÔ∏è\"]\n",
    "    #add a new column new_score that get the average of the selected criteria in df \n",
    "    else:\n",
    "        \n",
    "        for c in num_criteria:\n",
    "            this_df[\"new_score\"] += this_df[c]\n",
    "        \n",
    "        for s_tuple in str_criteria:\n",
    "        #     if s_tuple[1] == this_df[s_tuple[0]]:\n",
    "        #         this_df[\"str_score\"] += MAX_SCORE\n",
    "            this_df.loc[this_df[s_tuple[0]] == s_tuple[1], \"new_score\"] += MAX_SCORE\n",
    "        this_df[\"new_score\"] = this_df[\"new_score\"] / (len(num_criteria) + len(str_criteria))\n",
    "        this_df[\"new_score\"] = this_df[\"new_score\"] * confident + df[\"Average ‚¨ÜÔ∏è\"] * (1 - confident)\n",
    "\n",
    "    this_df = this_df.sort_values(by=[\"new_score\"], ascending=False)\n",
    "\n",
    "    \n",
    "    return this_df[\"eval_name\"][:num_top_model]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking of tech Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040        MaziyarPanahi_calme-3.2-instruct-78b_bfloat16\n",
      "1036        MaziyarPanahi_calme-3.1-instruct-78b_bfloat16\n",
      "2219              dfurman_CalmeRys-78B-Orpo-v0.1_bfloat16\n",
      "1030             MaziyarPanahi_calme-2.4-rys-78b_bfloat16\n",
      "2519    huihui-ai_Qwen2.5-72B-Instruct-abliterated_bfl...\n",
      "Name: eval_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tech industry\n",
    "tech_cri = [\"#Params (B)\"]\n",
    "\n",
    "tech_industry_models = [(\"Architecture\", \"GPTNeoXForCausalLM\"), (\"Architecture\", \"LlamaForCausalLM\"), (\"Architecture\", \"Qwen2ForCausalLM\"), (\"Architecture\", \"Qwen2MoeForCausalLM\"), (\"Architecture\",\"T5ForConditionalGeneration\"), (\"Architecture\", \"CohereForCausalLM\"), (\"Architecture\", \"GPTJForCausalLM\")]\n",
    "print(get_rank(tech_cri, confident=0.5, num_top_model=5, str_criteria = tech_industry_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking of legal industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040    MaziyarPanahi_calme-3.2-instruct-78b_bfloat16\n",
      "1036    MaziyarPanahi_calme-3.1-instruct-78b_bfloat16\n",
      "2219          dfurman_CalmeRys-78B-Orpo-v0.1_bfloat16\n",
      "1030         MaziyarPanahi_calme-2.4-rys-78b_bfloat16\n",
      "3267         newsbang_Homer-v1.0-Qwen2.5-72B_bfloat16\n",
      "Name: eval_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Legal industry\n",
    "legal_cri = [\"MMLU-PRO\", ]\n",
    "print(get_rank(legal_cri, confident=0.5, num_top_model=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manufacture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250                   dnhkng_RYS-Llama3.1-Large_bfloat16\n",
      "1696    VAGOsolutions_Llama-3.1-SauerkrautLM-70b-Instr...\n",
      "1040        MaziyarPanahi_calme-3.2-instruct-78b_bfloat16\n",
      "1426                   Sao10K_70B-L3.3-Cirrus-x1_bfloat16\n",
      "218           Daemontatox_Llama3.3-70B-CogniLink_bfloat16\n",
      "Name: eval_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "manufac_num_cri = [\"MMLU-PRO\", \"#Params (B)\"]\n",
    "best_architecture_for_manufacturing = [(\"Architecture\", \"LlamaForCausalLM\"), (\"Architecture\", \"GPTJForCausalLM\"), (\"Architecture\", \"CohereForCausalLM\"), (\"Architecture\", \"T5ForConditionalGeneration\"), (\"Architecture\", \"RwkvForCausalLM\")]\n",
    "#create another list that contain everything in best_architecture_for_manufacturing and (\"Type\", \"üî∂ fine-tuned on domain-specific datasets\")\n",
    "find_tune = [(\"Type\", \"üî∂ fine-tuned on domain-specific datasets\")]\n",
    "manu_str_cri = best_architecture_for_manufacturing + find_tune\n",
    "\n",
    "print(get_rank(manufac_num_cri, confident=0.5, num_top_model=5, str_criteria=manu_str_cri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1036    MaziyarPanahi_calme-3.1-instruct-78b_bfloat16\n",
      "2219          dfurman_CalmeRys-78B-Orpo-v0.1_bfloat16\n",
      "1030         MaziyarPanahi_calme-2.4-rys-78b_bfloat16\n",
      "1314               Qwen_Qwen2.5-72B-Instruct_bfloat16\n",
      "1013     MaziyarPanahi_calme-2.1-qwen2.5-72b_bfloat16\n",
      "Name: eval_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cs_num_cri = [\"IFEval\", \"MMLU-PRO\"]\n",
    "cs_str_cri = [(\"T\", \"üí¨\"), (\"Type\", \"üí¨ chat models (RLHF, DPO, IFT, ...)\")]\n",
    "\n",
    "print(get_rank(cs_num_cri, confident=0.5, num_top_model=5, str_criteria=cs_str_cri))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse151a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
